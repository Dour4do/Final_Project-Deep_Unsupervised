{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_8l9INMVQ0D"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Reshape, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, Conv2DTranspose\n",
        "from tensorflow.keras.layers import LeakyReLU, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load Fashion MNIST dataset\n",
        "(X_train, _), (_, _) = fashion_mnist.load_data()\n",
        "\n",
        "# Normalize dataset\n",
        "X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
        "X_train = np.expand_dims(X_train, axis=-1)\n",
        "\n",
        "# Set up GAN parameters\n",
        "img_rows = 28\n",
        "img_cols = 28\n",
        "channels = 1\n",
        "img_shape = (img_rows, img_cols, channels)\n",
        "latent_dim = 100\n",
        "\n",
        "# Build generator model\n",
        "def build_generator():\n",
        "    generator = Sequential([\n",
        "        Dense(128 * 7 * 7, input_dim=latent_dim),\n",
        "        LeakyReLU(alpha=0.01),\n",
        "        Reshape((7, 7, 128)),\n",
        "        Conv2DTranspose(128, kernel_size=3, strides=2, padding='same'),\n",
        "        LeakyReLU(alpha=0.01),\n",
        "        Conv2DTranspose(64, kernel_size=3, strides=1, padding='same'),\n",
        "        LeakyReLU(alpha=0.01),\n",
        "        Conv2DTranspose(channels, kernel_size=3, strides=2, padding='same', activation='tanh')\n",
        "    ])\n",
        "    return generator\n",
        "\n",
        "# Build discriminator model\n",
        "def build_discriminator():\n",
        "    discriminator = Sequential([\n",
        "        Conv2D(64, kernel_size=3, strides=2, input_shape=img_shape, padding='same'),\n",
        "        LeakyReLU(alpha=0.01),\n",
        "        Dropout(0.4),\n",
        "        Conv2D(128, kernel_size=3, strides=2, padding='same'),\n",
        "        LeakyReLU(alpha=0.01),\n",
        "        Dropout(0.4),\n",
        "        Flatten(),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    return discriminator\n",
        "\n",
        "# Compile both networks in the GAN\n",
        "def build_gan(generator, discriminator):\n",
        "    discriminator.compile(loss='binary_crossentropy',\n",
        "                          optimizer=Adam(learning_rate=0.0001, beta_1=0.5),\n",
        "                          metrics=['accuracy'])\n",
        "\n",
        "    # Combined model (stacked generator and discriminator)\n",
        "    discriminator.trainable = False\n",
        "    gan = Sequential([\n",
        "        generator,\n",
        "        discriminator\n",
        "    ])\n",
        "\n",
        "    gan.compile(loss='binary_crossentropy',\n",
        "                optimizer=Adam(learning_rate=0.0001, beta_1=0.5))\n",
        "\n",
        "    return gan\n",
        "\n",
        "# Plot generated images\n",
        "def plot_generated_images(generator, epoch, examples=25, dim=(5, 5), figsize=(10, 10)):\n",
        "    noise = np.random.normal(0, 1, (examples, latent_dim))\n",
        "    generated_images = generator.predict(noise)\n",
        "    generated_images = generated_images * 0.5 + 0.5  # Rescale images to 0-1 range\n",
        "\n",
        "    plt.figure(figsize=figsize)\n",
        "    for i in range(examples):\n",
        "        plt.subplot(dim[0], dim[1], i+1)\n",
        "        plt.imshow(generated_images[i, :, :, 0], cmap='gray')\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'gan_generated_image_epoch_{epoch}.png')\n",
        "    plt.show()\n",
        "\n",
        "# Train the GAN\n",
        "def train_gan(epochs, batch_size=128, save_interval=1):\n",
        "    # Build and compile the discriminator\n",
        "    discriminator = build_discriminator()\n",
        "\n",
        "    # Build and compile the generator\n",
        "    generator = build_generator()\n",
        "\n",
        "    # Build and compile GAN model\n",
        "    gan = build_gan(generator, discriminator)\n",
        "\n",
        "    # Training loop\n",
        "    batch_count = X_train.shape[0] // batch_size\n",
        "    d_losses = []\n",
        "    g_losses = []\n",
        "    for epoch in range(1, epochs+1):\n",
        "        print(f\"Epoch {epoch}/{epochs}\")\n",
        "\n",
        "        for _ in range(batch_count):\n",
        "            # Generate random noise as input for the generator\n",
        "            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "\n",
        "            # Generate fake images from the noise\n",
        "            generated_images = generator.predict(noise)\n",
        "\n",
        "            # Get a random set of real images\n",
        "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "            real_images = X_train[idx]\n",
        "\n",
        "            # Concatenate real and fake images to form the batch for discriminator\n",
        "            X = np.concatenate([real_images, generated_images])\n",
        "\n",
        "            # Labels for generated and real data\n",
        "            y_dis = np.zeros(2*batch_size)\n",
        "            y_dis[:batch_size] = 0.9  # One-sided label smoothing\n",
        "\n",
        "            # Train discriminator\n",
        "            discriminator.trainable = True\n",
        "            d_loss, d_acc = discriminator.train_on_batch(X, y_dis)\n",
        "\n",
        "            # Train generator\n",
        "            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "            y_gen = np.ones(batch_size)\n",
        "            discriminator.trainable = False\n",
        "            g_loss = gan.train_on_batch(noise, y_gen)\n",
        "\n",
        "        # Print discriminator loss, accuracy, and generator loss\n",
        "        print(\"[Discriminator loss: {:.6f}, accuracy.: {:.2f}%] [Generator loss: {:.6f}]\".format(d_loss, 100 * d_acc, g_loss))\n",
        "        d_losses.append(d_loss)\n",
        "        g_losses.append(g_loss)\n",
        "\n",
        "        # Plot generated images at save_interval\n",
        "        if epoch % save_interval == 0:\n",
        "            plot_generated_images(generator, epoch)\n",
        "\n",
        "    # Plot the losses\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(d_losses, label='Discriminator loss')\n",
        "    plt.plot(g_losses, label='Generator loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('GAN Loss')\n",
        "    plt.legend()\n",
        "    plt.savefig('gan_loss_plot.png')\n",
        "    plt.show()\n",
        "\n",
        "# Run the function to train the GAN\n",
        "def run_gan():\n",
        "    epochs = 50\n",
        "    batch_size = 128\n",
        "    save_interval = 1\n",
        "    train_gan(epochs=epochs, batch_size=batch_size, save_interval=save_interval)\n",
        "\n",
        "# Run the GAN\n",
        "run_gan()\n"
      ]
    }
  ]
}